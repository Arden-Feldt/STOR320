---
title: "Lab 10: Modeling Basics"
author: "FIRSTNAME LASTNAME"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F)
options(scipen=999)
library(tidyverse)
library(modelr)
library(broom)
```

# Introduction

In this lab, you will build predictive models for board game ratings. The dataset below was scraped from [boardgamegeek.com](www.boardgamegeek.com) and contains information on the top 4,999 board games. Below, you will see a preview of the data

```{r}
bgg<-read.csv("bgg.csv")
bgg2=bgg[,c(4:13,15:20)]
head(bgg2)
```


# Board Game Analysis

### Q1 (1.5 Points)

There are 16 variables and we want to create some more. Create a new dataframe called $bgg3$ where you use the mutate function to create the following variables:

- *duration=2018-year+1*
- *vote.per.year=num_votes/duration*
- *own.per.year=owned/duration*
- *player.range=max_players-min_players*
- *log_vote=log(num_votes+1)*
- *log_own=log(owned+1)*
- *diff_rating=avg_rating-geek_rating*

```{r,eval=T}

bgg3 <- bgg2 %>%
  mutate(
    duration = 2018 - year + 1,
    vote.per.year = num_votes / duration,
    own.per.year = owned / duration,
    player.range = max_players - min_players,
    log_vote = log(num_votes + 1),
    log_own = log(owned + 1),
    diff_rating = avg_rating - geek_rating
  )

head(bgg3)
```

**Question:** In complete sentences, what is the purpose of adding 1 for the log transformed variables?

Adding the one avoids you from taking the log of 0, which is undefined and could cause errors/problems.

**Question:** In complete sentences, what is the purpose of adding 1 in the creation of the year variable?

To avoid duration from being 0. Which one: does not make sense, and two: would be a huge issue when you divide by duration.
### Q2 (2 Points)

We hypothesize the geek rating increases when the number of votes increases and/or the ownership increases. Create four scatter plots showing the association with geek_rating and the following variables:

- *num_votes*
- *owned*
- *log_vote*
- *log_own*


```{r,eval=T}
plot_num_votes <- ggplot(bgg3, aes(x = num_votes, y = geek_rating)) +
  geom_point() +
  labs(x = "Number of Votes", y = "Geek Rating") +
  ggtitle("Geek Rating vs Number of Votes")

plot_owned <- ggplot(bgg3, aes(x = owned, y = geek_rating)) +
  geom_point() +
  labs(x = "Owned", y = "Geek Rating") +
  ggtitle("Geek Rating vs Ownership")

plot_log_vote <- ggplot(bgg3, aes(x = log_vote, y = geek_rating)) +
  geom_point() +
  labs(x = "Log(Number of Votes + 1)", y = "Geek Rating") +
  ggtitle("Geek Rating vs Log(Number of Votes + 1)")

plot_log_own <- ggplot(bgg3, aes(x = log_own, y = geek_rating)) +
  geom_point() +
  labs(x = "Log(Owned + 1)", y = "Geek Rating") +
  ggtitle("Geek Rating vs Log(Owned + 1)")

print(plot_num_votes)
print(plot_owned)
print(plot_log_vote)
print(plot_log_own)

```

**Question:** In complete sentences, describe how the relationship changes when you take the log of the independent variable.

 Loging helps linearize the plot. It stretches across the x axis, allowing us to see the trend better. In this case it looks mostly linear and increasing. Both have a slight positive curve to them too, something you cant see on the other plots.


### Q3 (0.5 Points)

Randomly sample approximately 80\% of the data in `bgg3` for a training dataset and the remaining will act as a test set. Call the training dataset `train.bgg` and the testing dataset `test.bgg`.

```{r,eval=T}
set.seed(1)

bgg4 <- bgg3 %>%
  mutate(Set = sample(c("Train", "Test"), size = n(), replace = TRUE, prob = c(0.8, 0.2)))

train.bgg <- filter(bgg4, Set == "Train")
test.bgg <- filter(bgg4, Set == "Test")

```



### Q4 (0.5 Points)

Now, we want to fit models to the training dataset. Use the `lm()` function to create 3 model objects in R called `lm1`, `lm2`, `lm3` based on the following linear models, respectively:

- $\textrm{geek_rating}=\beta_0+\beta_1 log(\textrm{num_votes})+\epsilon$
- $\textrm{geek_rating}=\beta_0+\beta_1 log(\textrm{owned})+\epsilon$
- $\textrm{geek_rating}=\beta_0+\beta_1 log(\textrm{owned})+ \beta_2 \textrm{vote.per.year}+ \beta_3 \textrm{weight} + \epsilon$

```{r,eval=T}
lm1 <- lm(geek_rating ~ log(num_votes), data = train.bgg)
lm2 <- lm(geek_rating ~ log(owned), data = train.bgg)
lm3 <- lm(geek_rating ~ log(owned) + vote.per.year + weight, data = train.bgg)
```

### Q5 (1 Point)

Add predictions and residuals for all 3 models to the test set. Create a new data frame called `test.bgg2` and give all your predictions and residuals different names. Use the `str()` function to show these variables were created


```{r,eval=T}



test.bgg2 <- test.bgg %>%
  mutate(
    prediction_lm1 = predict(lm1, newdata = test.bgg),
    residual_lm1 = test.bgg$geek_rating - prediction_lm1,
    prediction_lm2 = predict(lm2, newdata = test.bgg),
    residual_lm2 = test.bgg$geek_rating - prediction_lm2,
    prediction_lm3 = predict(lm3, newdata = test.bgg),
    residual_lm3 = test.bgg$geek_rating - prediction_lm3,
  )

str(test.bgg2)
```


### Q6 (0.5 Points)

Create a function called `MAE.func()` that returns the mean absolute error based on a vector of the residuals and test your function on the vector called `test`.

Solution 1:
```{r,eval=T}
test=c(-5,-2,0,3,5)

MAE.func <- function(residuals_vector) {
  mae <- mean(abs(residuals_vector))
  return(mae)
}

MAE.func(test)
```

### Q7 (1 Point)

Use your function on the `test.bgg2` to calculate the out-of-sample MAE of all three models based on the associated residuals. Make sure you display the mean absolute error from these different models in your output.

```{r,eval=T}
mae_lm1 <- MAE.func(test.bgg2$residual_lm1)
mae_lm2 <- MAE.func(test.bgg2$residual_lm2)
mae_lm3 <- MAE.func(test.bgg2$residual_lm3)

cat("MAE for lm1:", mae_lm1, "\n")
cat("MAE for lm2:", mae_lm2, "\n")
cat("MAE for lm3:", mae_lm3, "\n")

```

**Question:** Which model does the best job at predicting the geek rating of these board games?

lm3 is the model tHat does the best job at predicting the geek rating of the games. 

### Q8 (3 Points)

For the third model only, use 10-fold cross-validation and measure the out-of-sample mean absolute error. Print out the final cross-validated mean absolute error.

```{r,eval=T}

#OUT.MAE=matrix(NA,10,2)

model_three <- lm(geek_rating ~ log(owned) + vote.per.year + weight, data = train.bgg)
#model <- lm(sales ~., data = training_dataset)
 
folded = crossv_kfold(train.bgg, k = 10)
# predicting the target variable
#predictions <- predict(model, testing_dataset)
#prediction_lm3 = predict(lm3, newdata = test.bgg)
#residual_lm3 = test.bgg$geek_rating - prediction_lm3
 
#print(test.bgg)

result = folded %>%
  mutate(model = map(train, ~lm(model_three, data = .)),
         predict = map2(model, test, ~augment(.x, newdata = .y)),
         mae = map2_dbl(predict, test, ~mean(abs(.x$.resid))))

#residuals_lm3 <- prediction_lm3 - test.bgg$geek_rating

final_MAE <- mean(result$mae)

print(final_MAE)

#folded_data <- crossv_kfold(test.bgg2, 10) %>%
#  select(-`.id`)

#print(OUT.MAE)

#for(I in 1:nrow(OUT.MAE)){
#  for(J in 1:ncol(OUT.MAE)){
#    DATA3.PRED = test.bgg2 %>% 
#      mutate(predict = predict(lm3, newdata = test.bgg)) %>%
#      select(predict) %>%
#      unnest()
#    OUT.MAE[I, J] = MAE.func(predict = DATA3.PRED$.fitted)
#  }
#}

#print(OUT.MAE)
```



**Question:** What is the absolute difference between the out-of-sample mean absolute error measured using a test set and the mean absolute error measured using cross validation? When you type your answer in complete sentences use inline R code to calculate the absolute difference and input it directly into your sentence.

The absolute difference is: ```r abs(0.1696334 - 0.1725177)```.





